\documentclass[11pt]{article}
\usepackage{graphicx}
\usepackage{amsmath}

\begin{document}

\title{SPS Coursework 2 Report}
\author{Michael Mafeni and Ainesh Sevellaraja}
\date{}
\maketitle

\section{Feature Selection}
We first plotted a 13x13 scatter plot for all pairwise combinations of features of the training data set, as seen below.

%in Figure \ref{fig:13x13}.
  
%\begin{figure}
%\centering
\begin{center}
\includegraphics[scale=0.25]{feature_selection}
\end{center}
%\caption{13x13 plot of pairwise combinations of features}
%\label{fig:13x13}
%\end{figure}

Based on the plot above, we decided to select features 10 and 12 as the data separate into their classes fairly well. This can be seen in the scatter plot of feature 12 against feature 10 below. We then reduced the train set and test set to only contain features 10 and 12.
% in Figure \ref{fig:10and12}.

%\begin{figure}
%\centering
\begin{center}
\includegraphics[scale=0.3]{features_10_12}
\end{center}
%\caption{Features 10 and 12 isolated}
%\label{fig:10and12}
%\end{figure}

\section{K-Nearest Neighbours (KNN)}
We implemented the K-Nearest Neighbours classifier. By setting $k = \{1,2,3,4,5,7\}$ our classifier gave the following accuracies and confusion matrices when run with the reduced test set.

%\begin{table}
\begin{center}
\begin{tabular}{c|c|c}
\textbf{k} & \textbf{Accuracy}\\
\hline
1 & 0.7925\\
2 & 0.6792\\
3 & 0.7358\\
4 & 0.7358\\
5 & 0.6981\\
7 & 0.7358\\
\end{tabular}
\end{center}
%\end{table}

\begin{center}
\includegraphics[scale=0.2]{Confusion_matrix(k=1)}
\includegraphics[scale=0.2]{Confusion_matrix(k=2)}
\includegraphics[scale=0.2]{Confusion_matrix(k=3)}
\includegraphics[scale=0.2]{Confusion_matrix(k=4)}
\includegraphics[scale=0.2]{Confusion_matrix(k=5)}
\includegraphics[scale=0.2]{Confusion_matrix(k=7)}
\end{center}

As seen in the table, our classifier gives the highest accuracy when $k = 1$. This can also be seen in the confusion matrices as the values along the diagonals are greatest when $k = 1$. According to the confusion matrices, a larger portion of test samples in class 1 are correctly classified as $k$ increases, as the values in cell $(1,1)$ increase. Classes 2 and 3 on the other hand have fewer correctly-classified predictions as k increases, as the values in cells $(2,2)$ and $(3,3)$ follow a decreasing trend. As $k$ increases, a higher portion of test samples in class 3 are incorrectly-classified in class 2, especially when $k = 2$ where a higher portion is classified in class 2 than in class 3 since the value in cell $(3,2)$ exceeds the value in cell $(3,3)$. This may also be a reason why our classifier gives the lowest accuracy when $k = 2$.

The confusion matrices tell us that class 1 has the most data samples correctly predicted. This can also be seen in our scatter plot of feature 12 against feature 10 above, where the blue class which represents class 1 is visibly separated from the other two classes, which leads to a lower chance of class 1 being misclassified.

\section{Alternative Classifier}
For our alternative classifier, we have implemented the Naive Bayes classifier. We assumed the features selected to be independent and normally distributed. This classifier gives an accuracy of 0.8679, which is higher than that our KNN classifier. The classifier gives the following confusion matrix.

\begin{center}
\includegraphics[scale=0.2]{naive_bayes_matrix}
\end{center}

The confusion matrix tells us that all test data samples in class 1 were correctly classified. Hence, our assumption of the features being independent was reasonable for this as we obtained a higher accuracy than with KNN. This also strengthens our point of class 1 being more isolated from the other classes. We did not have to worry about smoothing as we assumed the features to be normally distributed, which meant the probabilities never reach 0.

\section{Use Three Features}
To obtain the third feature for our KNN classifier, we calculated the covariance matrices of the two already-selected features and every other feature. We picked the one that least correlates with the selected features.

$$
1 & 2 & 3
\begin{bmatrix} 
5.26\times10^{-2} & 1.93\times10^1 & -8.47\times10^{-3} \\
1.93\times10^1 & 9.59\times10^4 & -1.35\times10^1 \\
-8.47\times10^{-3} & -1.35\times10^1 & 1.53\times10^{-2}
\end{bmatrix}
$$

\includegraphics[scale=0.3]{3d_plot}

\section{Principal Component Analysis (PCA)}

\end{document}

