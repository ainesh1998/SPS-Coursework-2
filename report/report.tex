\documentclass[11pt]{article}
\usepackage{graphicx}

\begin{document}

\title{SPS Coursework 2 Report}
\author{Michael Mafeni and Ainesh Sevellaraja}
\date{}
\maketitle

\section{Feature Selection}
We first plotted a 13x13 scatter plot for all pairwise combinations of features of the training data set, as seen below.

%in Figure \ref{fig:13x13}.
  
%\begin{figure}
%\centering
\begin{center}
\includegraphics[scale=0.25]{feature_selection}
\end{center}
%\caption{13x13 plot of pairwise combinations of features}
%\label{fig:13x13}
%\end{figure}

Based on the plot above, we decided to select features 10 and 12 as the data separate into their classes fairly well. This can be seen in the scatter plot of feature 12 against feature 10 below. We then reduced the train set and test set to only contain features 10 and 12.
% in Figure \ref{fig:10and12}.

%\begin{figure}
%\centering
\begin{center}
\includegraphics[scale=0.3]{features_10_12}
\end{center}
%\caption{Features 10 and 12 isolated}
%\label{fig:10and12}
%\end{figure}

\section{K-Nearest Neighbours (KNN)}
We implemented the K-Nearest Neighbours classifier. By setting $k = \{1,2,3,4,5,7\}$ our classifier gave the following accuracies and confusion matrices when run with the reduced test set.

%\begin{table}
\begin{center}
\begin{tabular}{c|c|c}
\textbf{k} & \textbf{Accuracy}\\
\hline
1 & 0.7925\\
2 & 0.6792\\
3 & 0.7358\\
4 & 0.7358\\
5 & 0.6981\\
7 & 0.7358\\
\end{tabular}
\end{center}
%\end{table}

\begin{center}
\includegraphics[scale=0.2]{Confusion_matrix(k=1)}
\includegraphics[scale=0.2]{Confusion_matrix(k=2)}
\includegraphics[scale=0.2]{Confusion_matrix(k=3)}
\includegraphics[scale=0.2]{Confusion_matrix(k=4)}
\includegraphics[scale=0.2]{Confusion_matrix(k=5)}
\includegraphics[scale=0.2]{Confusion_matrix(k=7)}
\end{center}

As seen in the table, our classifier gives the highest accuracy when $k = 1$. This can also be seen in the confusion matrices as the values along the diagonals are greatest when $k = 1$. According to the confusion matrices, a larger portion of test samples in class 1 are correctly classified as $k$ increases, as the values in cell $(1,1)$ increase. Classes 2 and 3 on the other hand have fewer correctly-classified predictions as k increases, as the values in cells $(2,2)$ and $(3,3)$ follow a decreasing trend. As $k$ increases, a higher portion of test samples in class 3 are incorrectly-classified in class 2, especially when $k = 2$ where a higher portion is classified in class 2 than in class 3 since the value in cell $(3,2)$ exceeds the value in cell $(3,3)$. This may also be a reason why our classifier gives the lowest accuracy when $k = 2$.

\section{Alternative Classifier}
For our alternative classifier, we have implemented the Naive Bayes classifier. This classifier gives a higher accuracy than our KNN classifier.
\section{Use Three Features}

\section{Principal Component Analysis (PCA)}

\end{document}

